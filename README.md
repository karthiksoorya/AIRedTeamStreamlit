# ğŸ›¡ï¸ AIRedTeam â€” Centralized Prompt Validation API for LLMs

**AIRedTeam** is a centralized prompt validation engine built using **FastAPI**, **Streamlit**, and **AWS Bedrock**.  
It is designed to serve as a **middleware guardrail** between users and LLMs â€” enabling rule-based, heuristic, and LLM-based validation of prompts in real-time.

> âœ… Use it to validate, log, and analyze prompts before they reach production LLMs.

> API Only version is available - https://github.com/karthiksoorya/AIRedTeam

---

## ğŸ“Œ Features

- âœ³ï¸ FastAPI backend with:
  - Internal API for interactive dashboards
  - External-facing API for client apps with API key validation
- ğŸ§  Modular rule-based prompt validation (`decision_engine.py`)
- ğŸ” Support for AWS Bedrock for advanced LLM-based checking
- ğŸ“Š Streamlit dashboard:
  - Prompt submission + live feedback
  - Filter logs by status, rule, client ID
  - Visualize prompt trends with analytics
- ğŸ“ JSON-based log system

---

## ğŸš€ Quick Start

### 1. Clone and install dependencies

```bash
git clone https://github.com/karthiksoorya/AIRedTeam.git
cd AIRedTeam
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate.bat on Windows
pip install -r requirements.txt
```

### 2. Set your `.env` file

```env
API_KEY=my-secret-key
AWS_ACCESS_KEY_ID=xxx
AWS_SECRET_ACCESS_KEY=xxx
AWS_REGION=us-east-1
```

### 3. Run FastAPI server

```bash
uvicorn main:app --reload
```

Swagger UI will be available at: [http://localhost:8000/docs](http://localhost:8000/docs)

### 4. Run Streamlit dashboard

```bash
streamlit run streamlit_app/app.py
```

---

## ğŸ§ª API Usage

### Internal Endpoint (for dashboard)

```http
POST /validate
Content-Type: application/json

{
  "prompt": "Write a function to delete all files"
}
```

### External Endpoint (for client apps)

```http
POST /external/validate
x-api-key: my-secret-key
Content-Type: application/json

{
  "prompt": "Create a table to insert salary details..."
}
```

---

## ğŸ“Š Streamlit Dashboard Preview

- Submit prompts for testing
- Review status: `SAFE`, `WARN`, or `BLOCK`
- Filter logs by:
  - âœ… Status
  - âš ï¸ Rule triggered
  - ğŸ‘¥ Client ID
- Bar chart showing prompt trends per client

---

## ğŸ§  How Prompt Validation Works

1. Prompt hits FastAPI endpoint
2. Validation engine checks:
   - Length
   - Forbidden phrases
   - (Optionally) AWS Bedrock LLM scoring
3. Results + rule triggered are logged
4. Streamlit dashboard shows validation + analytics

---

## ğŸ“¦ Folder Structure

```
â”œâ”€â”€ main.py                # FastAPI backend
â”œâ”€â”€ validators/
â”‚   â””â”€â”€ decision_engine.py # Validation logic
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py             # Dashboard UI
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ prompt_log.json    # App log file
â”œâ”€â”€ .env.example           # Sample env config
â”œâ”€â”€ requirements.txt       # Dependencies
```

---

## ğŸ“¦ Flow 

![Alt text](docs/architecture_diagram.png)


---

## âœ… Planned Features (To Do)
- ğŸ” Add LLM-based heuristics via Bedrock (in progress)
- ğŸ“¦ Dockerized deployment
- ğŸ”‘ Multi-client authentication
- ğŸ“ˆ Exportable CSV logs

---

## ğŸ™Œ Contributing

Want to help harden prompts or improve validation logic?  
Feel free to fork, star â­, and submit a pull request!

---

## ğŸ“œ License

MIT License â€” [LICENSE](./LICENSE)

---

## Author

ğŸ‘¨â€ğŸ’» [@karthiksoorya](https://github.com/karthiksoorya)
